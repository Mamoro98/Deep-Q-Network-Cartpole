
# ğŸš€ Deep-Q-Network for CartPole ğŸ‹ï¸â€â™‚ï¸

This repository contains an implementation of a **Deep Q-Network (DQN)** to solve the **CartPole-v1** environment in Reinforcement Learning. The goal is for the agent to learn how to balance a pole on a cart for as long as possible. ğŸ› ï¸

## ğŸ“‚ Project Structure

```
Deep-Q-Network-Cartpole/
â”‚
â”œâ”€â”€ cartpole.ipynb    # Jupyter notebook with DQN implementation
â””â”€â”€ README.md         # Project overview (this file)
```

## ğŸ¯ Objectives

- Learn to solve the **CartPole-v1** environment using Deep Reinforcement Learning. ğŸ§ 
- Implement key components of DQN:
  - Q-Network ğŸ–¥ï¸
  - Replay Buffer ğŸ”
  - -greedy action selection ğŸ²
- Achieve a perfect score of 500 in the environment. ğŸ†

## ğŸ› ï¸ Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Mamoro98/Deep-Q-Network-Cartpole.git
   cd Deep-Q-Network-Cartpole
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the notebook**:
   Launch `cartpole.ipynb` in your favorite Jupyter environment:
   ```bash
   jupyter notebook cartpole.ipynb
   ```

## ğŸ§‘â€ğŸ’» Features

- **Dynamic Q-Learning**: Neural networks approximate the Q-value function. ğŸ§®
- **Replay Buffer**: Efficient experience replay to stabilize learning. ğŸ”„
- **Adaptive Exploration**: -greedy action selection with a decaying epsilon. ğŸŒŸ

## ğŸ“ˆ Performance

- The model is trained to solve CartPole by maximizing rewards over episodes.
- **Evaluation**: The agent achieves a consistent episode return of 500.
- Video: https://github.com/user-attachments/assets/326624dd-d382-4def-8e2d-3f3287535c5e



## ğŸ›ï¸ License

This project is licensed under the **Apache License 2.0**. See the `LICENSE` file for more details.

## ğŸ’¬ Acknowledgments

Special thanks to:
- **Arnu Pretorius**: Staff Research Scientist at InstaDeep. âœ¨
- **AIMS South Africa**: African Institute for Mathematical Sciences. ğŸŒ

---
